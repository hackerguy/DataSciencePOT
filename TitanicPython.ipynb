{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with Spark ML\n",
    "\n",
    "### In this notebook, we will explore Binary Classification using Spark ML. We will exploit Spark ML's high-level APIs built on top of DataFrames to create and tune machine learning pipelines. Spark ML Pipelines enable combining multiple algorithms into a single pipeline or workflow. We will heavitly utilize Spark ML's feature transformers to convert, modify and scale the features that will be used to develop the machine learning model. Finally, we will evaluate and cross validate our model to demonstrate the process of determining a best fit model.\n",
    "\n",
    "### The binary classification demo will utilize the famous Titanic dataset, which has been used for Kaggle competitions and can be downloaded here. There is no need to download the data manually as it is downloaded directly within the noteboook.\n",
    "https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "\n",
    "### The Titanic data set was chosen for this binary classification demonstration because it contains both text based and numeric features that are both continuous and categorical. This will give us the opportunity to explore and utilize a number of feature transformers available in Spark ML.\n",
    "     \n",
    "          \n",
    "               \n",
    "               \n",
    "    \n",
    "\n",
    "\n",
    "![IBM Logo](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSzlUYaJ9xykGC-N5PijcV_eDBGCXy_pMn7sy6ymrVypmJ22q5ZmA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Spark version and existence of Spark and Spark SQL contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spark version is 2.0.2.\n"
     ]
    }
   ],
   "source": [
    "print('The spark version is {}.'.format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Spark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 s659-33fa456b216cab-a5f39cf201a0 users 61194 Feb 20 18:46 Titanic.csv\r\n"
     ]
    }
   ],
   "source": [
    "!rm -f Titanic.csv\n",
    "!wget https://ibm.box.com/shared/static/crceca9g1ym3nl0hwaxa5c0j0m3e19l8.csv -O Titanic.csv -q\n",
    "!ls -l Titanic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data in as a DataFrame\n",
    "### Source data is in CSV format and includes a header. We will ask Spark to infer the schema/data types.\n",
    "### Drop unwanted columns and rows with null or invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadTitanicData = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Titanic.csv\")\n",
    "TitanicData = loadTitanicData.drop(\"PassengerId\").drop(\"Name\").drop(\"Ticket\").drop(\"Cabin\").dropna(how=\"any\", subset=(\"Age\", \"Embarked\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  We will use the 'Survived' column as a label for training the machine learning model\n",
    "#### Spark ML requires that that the labes are data type Double, so we will cast the  column as Double (it was inferred as Integer when read into Spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabeledTitanicData = (TitanicData.withColumn(\"SurvivedTemp\", TitanicData[\"Survived\"]\n",
    "    .cast(\"Double\")).drop(\"Survived\")\n",
    "    .withColumnRenamed(\"SurvivedTemp\", \"Survived\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Survived|\n",
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "|     1|female|19.0|    0|    2|26.2833|       S|     1.0|\n",
      "|     1|  male|65.0|    0|    0|  26.55|       S|     0.0|\n",
      "|     3|female|36.0|    1|    0|   17.4|       S|     1.0|\n",
      "|     3|  male|24.5|    0|    0|   8.05|       S|     0.0|\n",
      "|     1|  male|48.0|    1|    0|   52.0|       S|     1.0|\n",
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LabeledTitanicData.sample(False, 0.01, seed=0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of rows is 712.\n",
      "The number of rows labeled Not Survived is 424.\n",
      "The number of rows labeled Survived is 288.\n"
     ]
    }
   ],
   "source": [
    "print('The total number of rows is {}.'.format(LabeledTitanicData.count()))\n",
    "print('The number of rows labeled Not Survived is {}.'.format(LabeledTitanicData.filter(LabeledTitanicData['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived is {}.'.format(LabeledTitanicData.filter(LabeledTitanicData['Survived'] == 1).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the schema of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Survived: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LabeledTitanicData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringIndexer\n",
    "\n",
    "### StringIndexer is a transformer that encodes a string column to a column of indices. The indices are ordered by value frequencies, so the most frequent value gets index 0. If the input column is numeric, it is cast to string first. \n",
    "\n",
    "### For the Titanic data set, we will index the Sex/gender column as well as the Embarked column, which specifiies at which  port the passenger boarded the ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SexIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "EmbarkedIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketizer is a transformer that transforms a column of continuous features to a column of feature buckets, where the buckets are by a splits parameter. \n",
    "\n",
    "### For the Titanic data set, we will index the Age and Fare features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AgeBucketSplits = [0.0, 6.0, 12.0, 18.0, 40.0, 65.0, 80.0, float(\"inf\")]\n",
    "AgeBucket = Bucketizer(splits=AgeBucketSplits, inputCol=\"Age\", outputCol=\"AgeBucket\")\n",
    "\n",
    "FareBucketSplits = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 80.0, 100.0, float(\"inf\")]\n",
    "FareBucket = Bucketizer(splits=FareBucketSplits, inputCol=\"Fare\", outputCol=\"FareBucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorAssembler is a transformer that combines a given list of columns in the order specified into a single vector column in order to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols= [\"SexIndex\", \"EmbarkedIndex\", \"AgeBucket\", \"FareBucket\", \"SibSp\", \"Pclass\", \"Parch\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm\n",
    "### This normalization can help standardize your input data and improve the behavior of learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression is a popular method to predict a binary response\n",
    "### It is a special case of Generalized Linear models that predicts the probability of an outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"normFeatures\", labelCol=\"Survived\", predictionCol=\"prediction\", maxIter=10, regParam=0.01, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Pipeline is a sequence of stages where each stage is either a Transformer or an Estimator\n",
    "### These stages are run in order and the input DataFrame is transformed as it passes through each stage. \n",
    "\n",
    "### In machine learning, it is common to run a sequence of algorithms to process and learn from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[SexIndexer, EmbarkedIndexer, AgeBucket,FareBucket, assembler, normalizer, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly split the source data into training and test data sets\n",
    "### 90% training, 10% test\n",
    "### Cache the resulting DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of records in the traininig data set is 628.\n",
      "The number of rows labeled Not Survived in the training data set is 365.\n",
      "The number of rows labeled Survived in the training data set is 263.\n",
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|Survived|\n",
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "|     1|  male|36.0|    0|    0|26.2875|       S|     1.0|\n",
      "|     3|female|22.0|    0|    0|   7.75|       Q|     1.0|\n",
      "|     3|  male|16.0|    4|    1|39.6875|       S|     0.0|\n",
      "|     3|  male|27.0|    0|    0|  6.975|       S|     1.0|\n",
      "|     3|  male|31.0|    0|    0|  7.775|       S|     0.0|\n",
      "+------+------+----+-----+-----+-------+--------+--------+\n",
      "\n",
      "\n",
      "The number of records in the test data set is 84.\n",
      "The number of rows labeled Not Survived in the test data set is 59.\n",
      "The number of rows labeled Survived in the test data set is 25.\n",
      "+------+------+----+-----+-----+------+--------+--------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|  Fare|Embarked|Survived|\n",
      "+------+------+----+-----+-----+------+--------+--------+\n",
      "|     2|female|29.0|    0|    0|  10.5|       S|     1.0|\n",
      "|     2|  male|51.0|    0|    0|12.525|       S|     0.0|\n",
      "|     3|female| 4.0|    0|    2|22.025|       S|     1.0|\n",
      "|     3|female|24.0|    0|    0|  8.85|       S|     0.0|\n",
      "|     3|female|47.0|    1|    0|  14.5|       S|     0.0|\n",
      "+------+------+----+-----+-----+------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = LabeledTitanicData.randomSplit([90.0,10.0], seed=1)\n",
    "train.cache()\n",
    "test.cache()\n",
    "print('The number of records in the traininig data set is {}.'.format(train.count()))\n",
    "print('The number of rows labeled Not Survived in the training data set is {}.'.format(train.filter(train['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived in the training data set is {}.'.format(train.filter(train['Survived'] == 1).count()))\n",
    "train.sample(False, 0.01, seed=0).show(5)\n",
    "print('')\n",
    "\n",
    "print('The number of records in the test data set is {}.'.format(test.count()))\n",
    "print('The number of rows labeled Not Survived in the test data set is {}.'.format(test.filter(train['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived in the test data set is {}.'.format(test.filter(train['Survived'] == 1).count()))\n",
    "test.sample(False, 0.1, seed=0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the pipeline to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on document in the Test data set\n",
    "### Keep in mind that the model has not seen the data in the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----+-----+-----+------+--------+--------+--------+-------------+---------+----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|Pclass|   Sex| Age|SibSp|Parch|  Fare|Embarked|Survived|SexIndex|EmbarkedIndex|AgeBucket|FareBucket|            features|        normFeatures|       rawPrediction|         probability|prediction|\n",
      "+------+------+----+-----+-----+------+--------+--------+--------+-------------+---------+----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|     2|female|29.0|    0|    0|  10.5|       S|     1.0|     1.0|          0.0|      3.0|       1.0|[1.0,0.0,3.0,1.0,...|[0.14285714285714...|[-1.4372675609811...|[0.19196883906346...|       1.0|\n",
      "|     2|  male|51.0|    0|    0|12.525|       S|     0.0|     0.0|          0.0|      4.0|       1.0|(7,[2,3,5],[4.0,1...|(7,[2,3,5],[0.571...|[1.67924947176139...|[0.84280512322624...|       0.0|\n",
      "|     3|female| 4.0|    0|    2|22.025|       S|     1.0|     1.0|          0.0|      0.0|       2.0|[1.0,0.0,0.0,2.0,...|[0.125,0.0,0.0,0....|[-2.2352438350615...|[0.09662992385596...|       1.0|\n",
      "|     3|female|24.0|    0|    0|  8.85|       S|     0.0|     1.0|          0.0|      3.0|       0.0|(7,[0,2,5],[1.0,3...|(7,[0,2,5],[0.142...|[-0.8713809459553...|[0.29496703714114...|       1.0|\n",
      "|     3|female|47.0|    1|    0|  14.5|       S|     0.0|     1.0|          0.0|      4.0|       1.0|[1.0,0.0,4.0,1.0,...|[0.1,0.0,0.4,0.1,...|[-0.2178676561400...|[0.44574751352475...|       1.0|\n",
      "+------+------+----+-----+-----+------+--------+--------+--------+-------------+---------+----------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.sample(False, 0.1, seed=0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of predictions labeled Not Survived is 55.\n",
      "The number of predictions labeled Survived is 29.\n"
     ]
    }
   ],
   "source": [
    "print('The number of predictions labeled Not Survived is {}.'.format(predictions.filter(predictions['prediction'] == 0).count()))\n",
    "print('The number of predictions labeled Survived is {}.'.format(predictions.filter(predictions['prediction'] == 1).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+--------+------+-----+-----+--------+----------+\n",
      "|   Sex| Age|  Fare|Embarked|Pclass|Parch|SibSp|Survived|prediction|\n",
      "+------+----+------+--------+------+-----+-----+--------+----------+\n",
      "|female| 8.0|21.075|       S|     3|    1|    3|     0.0|       1.0|\n",
      "|  male|20.5|  7.25|       S|     3|    0|    0|     0.0|       0.0|\n",
      "|  male|26.0|7.8875|       S|     3|    0|    0|     0.0|       0.0|\n",
      "|  male|30.0|  8.05|       S|     3|    0|    0|     0.0|       0.0|\n",
      "|  male|33.0|   9.5|       S|     3|    0|    0|     0.0|       0.0|\n",
      "+------+----+------+--------+------+-----+-----+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------+----+--------+--------+------+-----+-----+--------+----------+\n",
      "|   Sex| Age|    Fare|Embarked|Pclass|Parch|SibSp|Survived|prediction|\n",
      "+------+----+--------+--------+------+-----+-----+--------+----------+\n",
      "|female|23.0| 113.275|       C|     1|    0|    1|     1.0|       1.0|\n",
      "|  male|35.0|512.3292|       C|     1|    0|    0|     1.0|       1.0|\n",
      "|  male|38.0|    90.0|       S|     1|    0|    1|     1.0|       1.0|\n",
      "|  male|42.0| 52.5542|       S|     1|    0|    1|     1.0|       0.0|\n",
      "|female|35.0|    21.0|       S|     2|    0|    0|     1.0|       1.0|\n",
      "+------+----+--------+--------+------+-----+-----+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(predictions.filter(\"Survived = 0.0\")\n",
    "     .select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\")\n",
    "     .sample(False, 0.1, seed=0).show(5))\n",
    "\n",
    "(predictions.filter(\"Survived = 1.0\")\n",
    "     .select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\")\n",
    "     .sample(False, 0.5, seed=0).show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create an evaluator for the binary classification using area under the ROC Curve as the evaluation metric\n",
    "\n",
    "### Receiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied\n",
    "\n",
    "The curve is created by plotting the true positive rate against the false positive rate at various threshold settings. The ROC curve is thus the sensitivity as a function of fall-out. The area under the ROC curve is useful for comparing and selecting the best machine learning model for a given data set. A model with an area under the ROC curve score near 1 has very good performance. A model with a score near 0.5 is about as good as flipping a coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve = 0.8389830508474577.\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Survived\").setMetricName(\"areaUnderROC\")\n",
    "print('Area under the ROC curve = {}.'.format(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters\n",
    "### Generate hyperparameter combinations by taking the cross product of some parameter values\n",
    "\n",
    "Spark ML algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by Spark ML itself. Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.1, 0.3])\n",
    "                 .addGrid(lr.elasticNetParam, [0.0, 1.0])\n",
    "                 .addGrid(normalizer.p, [1.0, 2.0])\n",
    "                 .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cross validator to tune the pipeline with the generated parameter grid\n",
    "Spark ML provides for cross-validation for hyperparameter tuning. Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluate the ML Pipeline to find the best model\n",
    "### using the area under the ROC evaluator and hyperparameters specified in the parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve for best fitted model = 0.8548955057651987.\n"
     ]
    }
   ],
   "source": [
    "cvModel = cv.fit(LabeledTitanicData)\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(LabeledTitanicData))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what improvement we achieve by tuning the hyperparameters using cross-evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under the ROC curve for non-tuned model = 0.8389830508474577.\n",
      "Area under the ROC curve for best fitted model = 0.8548955057651987.\n",
      "Improvement = 1.90%\n"
     ]
    }
   ],
   "source": [
    "print('Area under the ROC curve for non-tuned model = {}.'.format(evaluator.evaluate(predictions)))\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(LabeledTitanicData))))\n",
    "print('Improvement = {0:0.2f}%'.format((evaluator.evaluate(cvModel.transform(LabeledTitanicData)) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make improved predictions using the Cross-validated model\n",
    "### Using the Test data set and DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|     1.0|       1.0|\n",
      "|     0.0|       0.0|\n",
      "|     1.0|       1.0|\n",
      "|     0.0|       1.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvModel.transform(test).select(\"Survived\", \"prediction\").sample(False, 0.1, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like above, but now using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|     1.0|       1.0|\n",
      "|     0.0|       0.0|\n",
      "|     1.0|       1.0|\n",
      "|     0.0|       1.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "|     0.0|       0.0|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create temporary table\n",
    "cvModel.transform(test).createOrReplaceTempView(\"cvModelPredictions\")\n",
    "spark.sql(\"select Survived, prediction from cvModelPredictions\").sample(False, 0.1, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction on an imaginary passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the imaginary passenger's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+--------+------+-----+-----+\n",
      "|   Sex| Age|Fare|Embarked|Pclass|SibSp|Parch|\n",
      "+------+----+----+--------+------+-----+-----+\n",
      "|female|40.0|15.0|       C|     2|    1|    1|\n",
      "+------+----+----+--------+------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SexValue = 'female'\n",
    "AgeValue = 40.0\n",
    "FareValue = 15.0\n",
    "EmbarkedValue = 'C'\n",
    "PclassValue = 2\n",
    "SibSpValue = 1\n",
    "ParchValue = 1\n",
    "\n",
    "PredictionFeatures = (spark.createDataFrame([(SexValue, AgeValue, FareValue, EmbarkedValue, PclassValue, SibSpValue, ParchValue)],\n",
    "    ['Sex', 'Age', 'Fare', 'Embarked', 'Pclass', 'SibSp', 'Parch']))\n",
    "PredictionFeatures.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict whether the imaginary person would have survived\n",
    "### using the best fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------------------------------------+----------+\n",
      "|rawPrediction                             |probability                            |prediction|\n",
      "+------------------------------------------+---------------------------------------+----------+\n",
      "|[0.19683342469048395,-0.19683342469048395]|[0.5490500943973472,0.4509499056026528]|0.0       |\n",
      "+------------------------------------------+---------------------------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SurvivedOrNotPrediction = cvModel.transform(PredictionFeatures)\n",
    "SurvivedOrNotPrediction.select('rawPrediction', 'probability', 'prediction').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did NOT Survive\n"
     ]
    }
   ],
   "source": [
    "SurvivedOrNot = SurvivedOrNotPrediction.select(\"prediction\").first()[0]\n",
    "if SurvivedOrNot == 0.0:\n",
    "    print(\"Did NOT Survive\")\n",
    "elif(SurvivedOrNot == 1.0):\n",
    "    print(\"Did Survive!!!\")\n",
    "else:\n",
    "    print(\"Invalid Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a quick look at applying the feature engineering performed above to a Random Forest Model\n",
    "### Random forests are ensembles of decision trees. They combine many decision trees in order to reduce the risk of overfitting.\n",
    "### We won't do any hyperparamter tuning in this example, but just show how to create and evaluate the model using all default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer().setInputCol(\"Survived\").setOutputCol(\"indexedLabel\").fit(LabeledTitanicData)\n",
    "\n",
    "# Train a RandomForest model\n",
    "rf = RandomForestClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"features\").setNumTrees(20)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "\n",
    "# Create new Pipeline using the RandomForest model and all the same feature transformers used above for logistic regression\n",
    "pipelineRF = Pipeline().setStages([labelIndexer, SexIndexer, EmbarkedIndexer, AgeBucket, FareBucket, assembler, normalizer, rf, labelConverter])\n",
    "\n",
    "# Train model.\n",
    "modelRF = pipelineRF.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictionsRF = modelRF.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictionsRF.select(\"predictedLabel\", \"Survived\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluatorRF = MulticlassClassificationEvaluator().setLabelCol(\"Survived\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "accuracyRF = evaluatorRF.evaluate(predictionsRF)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracyRF))\n",
    "\n",
    "rfModel = modelRF.stages[7]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![IBM Logo](http://www-03.ibm.com/press/img/Large_IBM_Logo_TN.jpg)\n",
    "\n",
    "Rich Tarro  \n",
    "Big Data Architect, IBM Corporation  \n",
    "email: rtarro@us.ibm.com\n",
    "\n",
    "February 17, 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (Experimental) with Spark 2.0",
   "language": "python",
   "name": "python3-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}