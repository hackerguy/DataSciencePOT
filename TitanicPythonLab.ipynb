{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification with Spark ML Lab\n",
    "\n",
    "### In this notebook, we will explore Binary Classification using Spark ML. We will exploit Spark ML's high-level APIs built on top of DataFrames to create and tune machine learning pipelines. Spark ML Pipelines enable combining multiple algorithms into a single pipeline or workflow. We will heavitly utilize Spark ML's feature transformers to convert, modify and scale the features that will be used to develop the machine learning model. Finally, we will evaluate and cross validate our model to demonstrate the process of determining a best fit model.\n",
    "\n",
    "### The binary classification demo will utilize the famous Titanic dataset, which has been used for Kaggle competitions and can be downloaded here. There is no need to download the data manually as it is downloaded directly within the noteboook.\n",
    "https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "\n",
    "### The Titanic data set was chosen for this binary classification demonstration because it contains both text based and numeric features that are both continuous and categorical. This will give us the opportunity to explore and utilize a number of feature transformers available in Spark ML.\n",
    "     \n",
    "          \n",
    "               \n",
    "               \n",
    "    \n",
    "\n",
    "\n",
    "![IBM Logo](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSzlUYaJ9xykGC-N5PijcV_eDBGCXy_pMn7sy6ymrVypmJ22q5ZmA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Spark version and existence of Spark and Spark SQL contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The spark version is {}.'.format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required Spark libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!rm -f Titanic.csv\n",
    "!wget https://ibm.box.com/shared/static/crceca9g1ym3nl0hwaxa5c0j0m3e19l8.csv -O Titanic.csv -q\n",
    "!ls -l Titanic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data in as a DataFrame\n",
    "### Source data is in CSV format and includes a header. We will ask Spark to infer the schema/data types.\n",
    "### Drop unwanted columns and rows with null or invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadTitanicData = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Titanic.csv\")\n",
    "TitanicData = loadTitanicData.drop(\"PassengerId\").drop(\"Name\").drop(\"Ticket\").drop(\"Cabin\").dropna(how=\"any\", subset=(\"Age\", \"Embarked\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  We will use the 'Survived' column as a label for training the machine learning model\n",
    "#### Spark ML requires that that the labes are data type Double, so we will cast the  column as Double (it was inferred as Integer when read into Spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabeledTitanicData = (TitanicData.withColumn(\"SurvivedTemp\", TitanicData[\"Survived\"]\n",
    "    .cast(\"Double\")).drop(\"Survived\")\n",
    "    .withColumnRenamed(\"SurvivedTemp\", \"Survived\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the labeled data\n",
    "<div class=\"panel-group\" id=\"accordion-1\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-1\" href=\"#collapse-1\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-1\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n",
    "          LabeledTitanicData.show()<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print out a sample of the DataFrame 'LabeledTitanicData'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The total number of rows is {}.'.format(LabeledTitanicData.count()))\n",
    "print('The number of rows labeled Not Survived is {}.'.format(LabeledTitanicData.filter(LabeledTitanicData['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived is {}.'.format(LabeledTitanicData.filter(LabeledTitanicData['Survived'] == 1).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the schema of the data\n",
    "<div class=\"panel-group\" id=\"accordion-2\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-2\" href=\"#collapse-2\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-2\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n",
    "          LabeledTitanicData.printSchema()<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the DataFrame schema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringIndexer\n",
    "\n",
    "### StringIndexer is a transformer that encodes a string column to a column of indices. The indices are ordered by value frequencies, so the most frequent value gets index 0. If the input column is numeric, it is cast to string first. \n",
    "\n",
    "### For the Titanic data set, we will index the Sex/gender column as well as the Embarked column, which specifiies at which  port the passenger boarded the ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SexIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "EmbarkedIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bucketizer is a transformer that transforms a column of continuous features to a column of feature buckets, where the buckets are by a splits parameter. \n",
    "\n",
    "### For the Titanic data set, we will index the Age and Fare features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AgeBucketSplits = [0.0, 6.0, 12.0, 18.0, 40.0, 65.0, 80.0, float(\"inf\")]\n",
    "AgeBucket = Bucketizer(splits=AgeBucketSplits, inputCol=\"Age\", outputCol=\"AgeBucket\")\n",
    "\n",
    "FareBucketSplits = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 80.0, 100.0, float(\"inf\")]\n",
    "FareBucket = Bucketizer(splits=FareBucketSplits, inputCol=\"Fare\", outputCol=\"FareBucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorAssembler is a transformer that combines a given list of columns in the order specified into a single vector column in order to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols= [\"SexIndex\", \"EmbarkedIndex\", \"AgeBucket\", \"FareBucket\", \"SibSp\", \"Pclass\", \"Parch\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm\n",
    "### This normalization can help standardize your input data and improve the behavior of learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression is a popular method to predict a binary response\n",
    "### It is a special case of Generalized Linear models that predicts the probability of an outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"normFeatures\", labelCol=\"Survived\", predictionCol=\"prediction\", maxIter=10, regParam=0.01, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Pipeline is a sequence of stages where each stage is either a Transformer or an Estimator\n",
    "### These stages are run in order and the input DataFrame is transformed as it passes through each stage. \n",
    "\n",
    "### In machine learning, it is common to run a sequence of algorithms to process and learn from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[SexIndexer, EmbarkedIndexer, AgeBucket,FareBucket, assembler, normalizer, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly split the source data into training and test data sets\n",
    "### 90% training, 10% test\n",
    "### Cache the resulting DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = LabeledTitanicData.randomSplit([90.0,10.0], seed=1)\n",
    "train.cache()\n",
    "test.cache()\n",
    "print('The number of records in the traininig data set is {}.'.format(train.count()))\n",
    "print('The number of rows labeled Not Survived in the training data set is {}.'.format(train.filter(train['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived in the training data set is {}.'.format(train.filter(train['Survived'] == 1).count()))\n",
    "train.sample(False, 0.01, seed=0).show(5)\n",
    "print('')\n",
    "\n",
    "print('The number of records in the test data set is {}.'.format(test.count()))\n",
    "print('The number of rows labeled Not Survived in the test data set is {}.'.format(test.filter(train['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived in the test data set is {}.'.format(test.filter(train['Survived'] == 1).count()))\n",
    "test.sample(False, 0.1, seed=0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the pipeline to the training data\n",
    "<div class=\"panel-group\" id=\"accordion-3\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse-3\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n",
    "          model = pipeline.fit(train)<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data assigning the result to a variable called 'model'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on document in the Test data set\n",
    "### Keep in mind that the model has not seen the data in the test data set\n",
    "<div class=\"panel-group\" id=\"accordion-4\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse-4\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-4\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n",
    "          predictions = model.transform(test)<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data assigning the result to a variable called 'predictions'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions.sample(False, 0.1, seed=0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The number of predictions labeled Not Survived is {}.'.format(predictions.filter(predictions['prediction'] == 0).count()))\n",
    "print('The number of predictions labeled Survived is {}.'.format(predictions.filter(predictions['prediction'] == 1).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(predictions.filter(\"Survived = 0.0\")\n",
    "     .select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\")\n",
    "     .sample(False, 0.1, seed=0).show(5))\n",
    "\n",
    "(predictions.filter(\"Survived = 1.0\")\n",
    "     .select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\")\n",
    "     .sample(False, 0.5, seed=0).show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create an evaluator for the binary classification using area under the ROC Curve as the evaluation metric\n",
    "\n",
    "### Receiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied\n",
    "\n",
    "The curve is created by plotting the true positive rate against the false positive rate at various threshold settings. The ROC curve is thus the sensitivity as a function of fall-out. The area under the ROC curve is useful for comparing and selecting the best machine learning model for a given data set. A model with an area under the ROC curve score near 1 has very good performance. A model with a score near 0.5 is about as good as flipping a coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Survived\").setMetricName(\"areaUnderROC\")\n",
    "print('Area under the ROC curve = {}.'.format(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Hyperparameters\n",
    "### Generate hyperparameter combinations by taking the cross product of some parameter values\n",
    "\n",
    "Spark ML algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by Spark ML itself. Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.1, 0.3])\n",
    "                 .addGrid(lr.elasticNetParam, [0.0, 1.0])\n",
    "                 .addGrid(normalizer.p, [1.0, 2.0])\n",
    "                 .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cross validator to tune the pipeline with the generated parameter grid\n",
    "Spark ML provides for cross-validation for hyperparameter tuning. Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluate the ML Pipeline to find the best model\n",
    "### using the area under the ROC evaluator and hyperparameters specified in the parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel = cv.fit(LabeledTitanicData)\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(LabeledTitanicData))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what improvement we achieve by tuning the hyperparameters using cross-evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Area under the ROC curve for non-tuned model = {}.'.format(evaluator.evaluate(predictions)))\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(LabeledTitanicData))))\n",
    "print('Improvement = {0:0.2f}%'.format((evaluator.evaluate(cvModel.transform(LabeledTitanicData)) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make improved predictions using the Cross-validated model\n",
    "### Using the Test data set and DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cvModel.transform(test).select(\"Survived\", \"prediction\").sample(False, 0.1, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like above, but now using SQL\n",
    "<div class=\"panel-group\" id=\"accordion-5\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-4\" href=\"#collapse-5\">\n",
    "        Hint</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse-5\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Type (or copy) the following in the cell below: <br>\n",
    "            spark.sql(\"select Survived, prediction from cvModelPredictions\").sample(False, 0.1, seed=0).show(10)<br>\n",
    "      </div>\n",
    "    </div>\n",
    "  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create temporary table\n",
    "cvModel.transform(test).createOrReplaceTempView(\"cvModelPredictions\")\n",
    "# use Spark SQL to query this temporary table to see the same columns as using the DataFrame APIs above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a prediction on an imaginary passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the imaginary passenger's features\n",
    "Try other combinations of features to see if you imaginary passenger would have survived the Titanic!\n",
    "Please make sure to not change any data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SexValue = 'female'\n",
    "AgeValue = 40.0\n",
    "FareValue = 15.0\n",
    "EmbarkedValue = 'C'\n",
    "PclassValue = 2\n",
    "SibSpValue = 1\n",
    "ParchValue = 1\n",
    "\n",
    "PredictionFeatures = (spark.createDataFrame([(SexValue, AgeValue, FareValue, EmbarkedValue, PclassValue, SibSpValue, ParchValue)],\n",
    "    ['Sex', 'Age', 'Fare', 'Embarked', 'Pclass', 'SibSp', 'Parch']))\n",
    "PredictionFeatures.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict whether the imaginary person would have survived\n",
    "### using the best fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SurvivedOrNotPrediction = cvModel.transform(PredictionFeatures)\n",
    "SurvivedOrNotPrediction.select('rawPrediction', 'probability', 'prediction').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SurvivedOrNot = SurvivedOrNotPrediction.select(\"prediction\").first()[0]\n",
    "if SurvivedOrNot == 0.0:\n",
    "    print(\"Did NOT Survive\")\n",
    "elif(SurvivedOrNot == 1.0):\n",
    "    print(\"Did Survive!!!\")\n",
    "else:\n",
    "    print(\"Invalid Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (Experimental) with Spark 2.0",
   "language": "python",
   "name": "python3-spark20"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}